




--Instructor Session :
  --Binary Search Algorithm
  ---Revision of Sorting Algo on 27th Aug






Time Complexity :
  TC is a metric used to determine the efficiency of algorithms....to compare the efficiency

  Why do we in particular have chose size of input as the varying factor to calculate the TC?
    --O(N), O(f(N))....N == size of the input over which my algorithm is running..
    --The size of input is the only factor of the algorithm that is HARDWARE AGNOSTIC

    --We are calculating the number of iterations/operations that are performed by the algorithm as a
        function of N(size of input)

    --Whenever we are calculating the time complexity, we often think about WORST-CASE.
        Because the input is not in the programmers control. It is unpredictable.
        So we consider the worst-case so that we can accordingly generalise the algorithmic analysis between various algorithms.
    --When we say WORST_CASE, we also increase the size of the input.
    i.e. to say we assume our N to be very large number

    --Constants add unnecessary complexity to the overall formula without adding much value.
      --So we ignore constants from the overall formula

    --We ignore the less significant terms from the time complexity formula.

  LogN Complexity :
  -- 3 raised to the power of 2 gives us 9
  -- log9 to base 2  == ~3

  --Merge Sort : N operations at each level * No. Of levels
    For this example : log7 to the base 2 = 3

  --Binary Search complexity analysis :
      To search in a sorted collection, what we do is try to eliminate certain set of elements after each step.
      This helps to get closer to the solution.
      Find the middle value, compare it with toFindValue, and accordingly throw away half of the current set of elements Until we find our element in the array
      So, here, like any other logarithmic complexity algo, we see a division by 2.
      Search space reduces by half every time.



Agenda for our next session :
  --Exception Handling
